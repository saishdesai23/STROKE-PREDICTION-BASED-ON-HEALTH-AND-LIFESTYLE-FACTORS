---
title: |
    | Methods of Data Science: Final Project 
    | Stroke Prediction based on health and lifestyle factors
    
author: 'Saish Desai, Anish Shetty, Vishnupriya Singh'
output: pdf_document 
---
# Importing all the libraries
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# installation of all the packages
# install.packages("ggstatsplot")

# loading all the packages
library("ggstatsplot") 
library(dplyr)
library(mltools)
library(data.table)
library(ROSE)
library(rpart)
library(tree)
library(randomForest)
library(caret)
library(MASS)
library(xgboost)
library(imbalance)
```

# Introduction

A stroke occurs when the oxygen and nutrient supply to the brain is interrupted by either a blocked artery or the leaking/bursting of a blood vessel. When that happens, the brain cannot get the blood and oxygen it needs, due to which the brain cells die. According to the American Stroke Organization, Stroke is the No 5. Cause of death and the leading cause of disability in the United States, and may depend on a person’s health, habits, and lifestyle. For this project, we have taken a dataset from Kaggle with predictors representing all these causes of stroke with an aim to predict whether a person is likely to get a stroke. The dataset has ‘5110’ observations, where each observation corresponds to a single person. The dataset has 12 variables – 11 independent variables/predators and 1 binary response variable. The outcome is ‘1’ if the person gets a stroke and ‘0’ otherwise.

# Research Questions

1) Given the information of a person from the list of predictors, can we predict if the person is likely to get a stroke?

2) Which factors are important in influencing whether a person is likely to get a stroke or not?

3) Will creating a balanced data set will give better prediction results

# Data Cleaning & Analysis

### Dataset Source : https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
```{r}
stroke_data = read.csv("healthcare-dataset-stroke-data.csv")
head(stroke_data)
```

### Dataset Description and EDA
```{r}
str(stroke_data)
```
### Understanding the distribution of variables.

Categorical Variables

### 1. Gender

```{r}
gender_counts <- as.data.frame(table(stroke_data$gender))

ggplot(gender_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Gender", y = "Frequency")
```

### 2. Hypertension
```{r}
ht_counts <- as.data.frame(table(stroke_data$hypertension))
ht_counts$Var1 <- if_else(ht_counts$Var1 == 0, 'No','Yes')

ggplot(ht_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Hypertension", y = "Frequency")


hypstr_counts <- as.data.frame(table(stroke_data$hypertension, stroke_data$stroke))
hypstr_counts$Var1 <- if_else(hypstr_counts$Var1 == 0, 'No','Yes')
hypstr_counts$Var2 <- if_else(hypstr_counts$Var2 == 0, 'No','Yes')

ggplot(hypstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Hypertension", y = "Frequency", fill = 'Stroke')
```


### 3. Heart Disease

```{r}
hd_counts <- as.data.frame(table(stroke_data$heart_disease))
hd_counts$Var1 <- if_else(hd_counts$Var1 == 0, 'No','Yes')

ggplot(hd_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Heart Disease", y = "Frequency")


hdstr_counts <- as.data.frame(table(stroke_data$heart_disease, stroke_data$stroke))
hdstr_counts$Var1 <- if_else(hdstr_counts$Var1 == 0, 'No','Yes')
hdstr_counts$Var2 <- if_else(hdstr_counts$Var2 == 0, 'No','Yes')

ggplot(hdstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Heart Disease", y = "Frequency", fill = 'Stroke')
```

### 4. Ever Married

```{r}

m_counts <- as.data.frame(table(stroke_data$ever_married))

ggplot(m_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Married?", y = "Frequency")


mstr_counts <- as.data.frame(table(stroke_data$ever_married, stroke_data$stroke))
mstr_counts$Var2 <- if_else(mstr_counts$Var2 == 0, 'No','Yes')

ggplot(mstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Marriage", y = "Frequency", fill = 'Stroke')

```

### 5. Work Type


```{r}

wt_counts <- as.data.frame(table(stroke_data$work_type))

ggplot(wt_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Work Type", y = "Frequency")


wtstr_counts <- as.data.frame(table(stroke_data$work_type, stroke_data$stroke))
wtstr_counts$Var2 <- if_else(wtstr_counts$Var2 == 0, 'No','Yes')

ggplot(wtstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Marriage", y = "Frequency", fill = 'Stroke')

```

### 6. Residence Type

```{r}

rt_counts <- as.data.frame(table(stroke_data$Residence_type))

ggplot(rt_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")


rtstr_counts <- as.data.frame(table(stroke_data$Residence_type, stroke_data$stroke))
rtstr_counts$Var2 <- if_else(rtstr_counts$Var2 == 0, 'No','Yes')

ggplot(rtstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency", fill = 'Stroke')

```

### 7. Smoking Status
```{r}

ss_counts <- as.data.frame(table(stroke_data$smoking_status))

ggplot(ss_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")


ssstr_counts <- as.data.frame(table(stroke_data$smoking_status, stroke_data$stroke))
ssstr_counts$Var2 <- if_else(ssstr_counts$Var2 == 0, 'No','Yes')

ggplot(ssstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Smoking Status", y = "Frequency", fill = 'Stroke')

```

# Continous Variables

### 8. Age
```{r}
#reference: https://www.r-bloggers.com/2021/11/how-to-make-stunning-histograms-in-r-a-complete-guide-with-ggplot2/


ggplot(stroke_data, aes(x=age)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```
### 9. Avg Glucose Level
```{r}

ggplot(stroke_data, aes(x=avg_glucose_level)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```



### 10. BMI
```{r}
#reference: https://www.r-bloggers.com/2021/11/how-to-make-stunning-histograms-in-r-a-complete-guide-with-ggplot2/
stroke_data$bmi <- as.integer(stroke_data$bmi)

ggplot(stroke_data, aes(x=bmi)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```

### 11. Stroke
```{r}
s_counts <- as.data.frame(table(stroke_data$stroke))

ggplot(s_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")
```
As we can see from the above graph, Our dependant variable stroke is highly imbalanced, Which is true in a realistic scenario as for a sample population, Number of people suffering from stroke is less.

Factorizing all the categorical variables

```{r}
# reference for one hot coding - https://datatricks.co.uk/one-hot-encoding-in-r-three-simple-methods

stroke_data$gender = factor(stroke_data$gender,levels = c('Male', 'Female'),labels = c(0,1))
stroke_data$ever_married = factor(stroke_data$ever_married,levels = c('No', 'Yes'),labels = c(0,1))
stroke_data$Residence_type = factor(stroke_data$Residence_type,levels = c('Rural', 'Urban'),labels = c(0,1))

stroke_data$hypertension = factor(stroke_data$hypertension,levels = c('0', '1'),labels = c(0,1))

stroke_data$heart_disease = factor(stroke_data$heart_disease,levels = c('0', '1'),labels = c(0,1))

stroke_data$smoking_status = factor(stroke_data$smoking_status,levels = c("formerly smoked", "never smoked", "smokes","Unknown"),labels = c(0,1,2,3))
stroke_data$work_type = factor(stroke_data$work_type,levels = c("children", "Govt_job", "Never_worked", "Private","Self-employed"),labels = c(0,1,2,3,4))

stroke_data['bmi'] <- as.numeric(stroke_data$bmi)
stroke_data$stroke<-as.factor(stroke_data$stroke)
```


### Check for NA's in the Dataset
```{r}
#Columnwise percentage of rows which are NA
colMeans(is.na(stroke_data))*100
```


```{r}
mean_bmi <- mean(na.omit(stroke_data$bmi))
median_bmi <- median(na.omit(stroke_data$bmi))

# estimating the mode value of the bmi column
# reference - https://www.tutorialspoint.com/r/r_mean_median_mode.htm

getmode <- function(v) {
   uniqv <- unique(v)
   index <-which.max(tabulate(match(v, uniqv))) #index of the most occuring value
   uniqv[index]
}

mode_bmi <- getmode(na.omit(stroke_data$bmi))


print(mean_bmi)
print(median_bmi)
print(mode_bmi)

# distribution for BMI
d <- density(na.omit(stroke_data$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(stroke_data$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(stroke_data$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")

```

### Imputation of missing values with mean of the values from the column 'bmi'
```{r}
stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
stroke_data <- na.omit(stroke_data)
str(stroke_data)
```
### Checking for outliers

```{r}
#boxplot before outlier removal
boxplot(stroke_data$age, main="Age distribution",
   xlab="ID", ylab="Age")

boxplot(stroke_data$bmi, main="Bmi distribution",
   xlab="ID", ylab="bmi")

boxplot(stroke_data$avg_glucose_level, main="Avg glucose level distribution",
   xlab="ID", ylab="avg_glucose_level")
```

### Outlier removal based on IQR
```{r}
#IQR

Q_age <- quantile(stroke_data$age, probs=c(.25, .75), na.rm = FALSE)
iqr_age <- IQR(stroke_data$age)

Q_bmi<- quantile(stroke_data$bmi, probs=c(.25, .75), na.rm = FALSE)
iqr_bmi <- IQR(stroke_data$bmi)
 
Q_avg_glucose_level<- quantile(stroke_data$avg_glucose_level, probs=c(.25, .75), na.rm = FALSE)
iqr_avg_glucose_level <- IQR(stroke_data$avg_glucose_level)


stroke_data_clean<- subset(stroke_data, 
                    stroke_data$age > (Q_age[1] - 1.5*iqr_age) & 
                    stroke_data$age < (Q_age[2] + 1.5*iqr_age) & 
                    stroke_data$bmi > (Q_bmi[1] - 1.5*iqr_bmi) & 
                    stroke_data$bmi < (Q_bmi[2] + 1.5*iqr_bmi) & 
                    stroke_data$avg_glucose_level > (Q_avg_glucose_level[1] - 1.5*iqr_avg_glucose_level) &
                    stroke_data$avg_glucose_level < (Q_avg_glucose_level[2]+1.5*iqr_avg_glucose_level))

str(stroke_data_clean)
str(stroke_data)
```


```{r}
#boxplot after outlier removal
boxplot(stroke_data_clean$age, main="Age distribution",
   xlab="ID", ylab="Age")

boxplot(stroke_data_clean$bmi, main="Bmi distribution",
   xlab="ID", ylab="bmi")

boxplot(stroke_data_clean$avg_glucose_level, main="Avg glucose level distribution",
   xlab="ID", ylab="avg_glucose_level")
```

# Modeling

# splitting the data set into train and test set
```{r}
# train test split
train <- sample(1:nrow(stroke_data_clean),nrow(stroke_data_clean)*0.7)
train_data <- stroke_data_clean[train, ]
test_data <- stroke_data_clean[-train, ]
```

# Random Forest on unbalanced data set
```{r}
rf <- randomForest(stroke ~., data = train_data, mtry = sqrt(ncol(train_data) - 1), ntree = 500)

# predicting the income value
yhat_rf <- predict(rf, test_data[,-12])

# accuracy
acc_rf = mean(yhat_rf == test_data$stroke)

# classification metrics
cm_rf <- confusionMatrix(yhat_rf, test_data$stroke, mode = "everything", positive="1")
cm_rf
```
Due to a high class imbalance all the entries are classified as belonging to class 0. This will lead to Precision and Recall value of "0", thus making the F1-score undefined.

# Oversampling to handle the imbalance in the data

# Using ovun.sample over sampling technique
```{r}
# data <- ROSE(stroke~., stroke_data, p=0.5, hmult.majo=1, hmult.mino=1)$data
data_balanced <- ovun.sample(stroke~ ., data = train_data, p=0.3, method = "over")$data
table(data_balanced$stroke)
```

The data is highly imbalanced due to the fact the there are very few people having suffered from stroke. So to reduce the class imbalance we use over smapling techniques to replicate some data with affecting the probabilty distribution of predictors in the data set. 


## Distribution of Numerical variables in the balanced Dataset
```{r}
# distribution of balanced data

# distribution for BMI
d <- density(na.omit(data_balanced$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(data_balanced$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(data_balanced$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")
```

```{r}

wn = sum(data_balanced$stroke =="0")/length(data_balanced$stroke)
wy = 1

rf_balanced <- randomForest(stroke ~., data = data_balanced, 
                   mtry =sqrt(ncol(data_balanced) - 1), 
                   classwt = c("0"=wn, "1"=wy), ntree = 500)


# predicting the income value
yhat_rf_balanced <- predict(rf_balanced, test_data[,-14])

# accuracy
acc_rf_balanced = mean(yhat_rf_balanced == test_data$stroke)

# classification metrics
cm_rf_balanced <- confusionMatrix(yhat_rf_balanced, test_data$stroke, mode = "everything", positive="1")
cm_rf_balanced
```
# SMOTE
Another way to sample the data is SMOTE which results in minority sampling. 
Applying the same on training data, we get the following results.
Before that, data should be converted to numeric.

```{r}
cols<- c("gender", "hypertension", "heart_disease", "ever_married", "work_type", "Residence_type","smoking_status" )
train_data[cols]<-lapply(train_data[cols], as.numeric)
test_data[cols]<-lapply(test_data[cols], as.numeric)
```

```{r}
data_balanced_smote<-oversample(train_data, ratio = 0.6, method = "SMOTE", classAttr = "stroke")
table(data_balanced_smote$stroke)
```

Here SMOTE adds few rpws to the training dataset, reproducing minority(1) factor level.

#Distribution of Numerical variables after Balancing the dataset

```{r}
# distribution for BMI
d <- density(na.omit(data_balanced_smote$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(data_balanced_smote$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(data_balanced_smote$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")
```

# Random Forest on the Balanced Dataset
```{r}
library(randomForest)

# modelling
data_balanced_smote$stroke<-as.factor(data_balanced_smote$stroke)
rf_smote <- randomForest(stroke ~., data = data_balanced_smote, mtry =sqrt(ncol(data_balanced_smote) - 1), ntree = 500)

# predicting the income value
yhat_rf_smote <- predict(rf_smote, test_data)

# accuracy
acc_rf_smote = mean(yhat_rf_smote == test_data$stroke)

# confusion matrix
cm_rf_smote <- confusionMatrix(yhat_rf_smote, as.factor(test_data$stroke), mode = "everything", positive="1")
cm_rf_smote
```
# Applying bagging and boosting
```{r}
data_balanced_smote$stroke<-as.factor(data_balanced_smote$stroke)
bagging <- randomForest(stroke ~., data = data_balanced_smote, mtry =ncol(data_balanced_smote) - 1, ntree = 500)

# predicting the income value
yhat_bagging <- predict(bagging, test_data)

# accuracy
acc_bagging = mean(yhat_bagging == test_data$stroke)

# confusion matrix
cm_bagging <- confusionMatrix(yhat_bagging, as.factor(test_data$stroke), mode = "everything", positive="1")
cm_bagging
```
 
# Boosting
```{r}
set.seed(1)
data_balanced_smote$stroke=as.numeric(data_balanced_smote$stroke)
data_balanced_smote$stroke = as.numeric(ifelse(data_balanced_smote$stroke == 1, "0", "1"))

library (gbm)
boost.data <- gbm(stroke~ ., data = data_balanced_smote,
distribution = "bernoulli", n.trees = 500,
interaction.depth = 4)
a = summary(predict(boost.data,test_data))
a

# prediction
yhat.boost.test <- ifelse(predict(boost.data ,newdata = test_data, n.trees =500)>a[4],1,0) #min+max/2

# accuracy
acc_boost = mean(yhat.boost.test == test_data$stroke)

# confusion matrix
cm_boost <- confusionMatrix(as.factor(yhat.boost.test), as.factor(test_data$stroke), mode = "everything", positive="1")
cm_boost
```
# SVM on Unbalanced Dataset
```{r}
library(e1071)
train_data$stroke<- as.factor(train_data$stroke)
levels(train_data$stroke)=c(0,1)
# model fitting
svmfit <- svm (stroke ~ ., 
               data = train_data,
               kernel = "linear",
               cost = 0.5, 
               scale = FALSE)

# prediction
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
test.pred <- predict (svmfit , test_data)

# accuracy
acc_svm = mean(test.pred == test_data$stroke)

# confusion matrix
cm_svm <- confusionMatrix(test.pred, test_data$stroke, mode = "everything", positive="1")
cm_svm


```

# SVM on Balanced Dataset
```{r}
library(e1071)
data_balanced_smote$stroke<- as.factor(data_balanced_smote$stroke)
levels(data_balanced_smote$stroke)=c(0,1)
set.seed (1)
# model fitting
svmfit <- svm (stroke ~ ., 
               data = data_balanced_smote,
               kernel = "linear",
               cost = 0.5, 
               scale = FALSE)

# prediction
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
test.pred_balanced  <- predict (svmfit , test_data)

# accuracy
acc_svm_balanced = mean(test.pred_balanced == test_data$stroke)

# confusion matrix
cm_svm_balanced <- confusionMatrix(test.pred_balanced, test_data$stroke, mode = "everything", positive="1")
cm_svm_balanced


```

# Naive Bayes on Unbalanced Dataset by using prior = 1
```{r}
set.seed(1)
library(e1071)

nbfit <- naiveBayes(stroke ~ ., data = train_data, prior = 1)
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)

# prediction
test.pred <- predict (nbfit , test_data)

# accuracy
acc_nb = mean(test.pred == test_data$stroke)
acc_nb

# confusion matrix
cm_nb <- confusionMatrix(test.pred, as.factor(test_data$stroke), mode = "everything", positive="1")
cm_nb
```


# Naive Bayes on Balanced Dataset by using prior = 1
```{r}
set.seed(1)
library(e1071)

nbfit_balanced <- naiveBayes ( stroke ~ ., data = data_balanced_smote, prior = 1)
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)


nbfit_balanced_pred <- predict(nbfit_balanced , test_data)

# accuracy
acc_nb_balanced = mean(nbfit_balanced_pred == test_data$stroke)
acc_nb_balanced

# confusion matrix
cm_nb_balanced <- confusionMatrix(test.pred, as.factor(test_data$stroke), mode = "everything", positive="1")
cm_nb_balanced
```



## LDA on Unbalanced Dataset

```{r}
lda_unbalanced <- lda(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_data)

lda_unbalanced

lda_unbalanced_predict <- predict(lda_unbalanced, test_data)
lda_class <- lda_unbalanced_predict$class

table(lda_class , test_data$stroke)
mean(lda_unbalanced_predict$class != test_data$stroke)

test_data$stroke = as.factor(test_data$stroke)
cm_lda <- confusionMatrix(lda_unbalanced_predict$class, test_data$stroke, mode = "everything", positive="1")
cm_lda
```



## LDA on Balanced dataset

```{r}
lda_balanced <- lda(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = data_balanced_smote)

lda_balanced

lda_balanced_predict <- predict(lda_balanced, test_data)
lda_class <- lda_balanced_predict$class

table(lda_class , test_data$stroke)
mean(lda_balanced_predict$class != test_data$stroke)

test_data$stroke = as.factor(test_data$stroke)
cm_lda_balanced <- confusionMatrix(lda_balanced_predict$class, test_data$stroke, mode = "everything", positive="1")
cm_lda_balanced
```

#Logistic on Unbalanced Dataset

```{r}
log_unbalanced = glm(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_data, family="binomial")

summary(log_unbalanced)

log_unbalanced_pred = predict(log_unbalanced,test_data,type='response')
log_unbalanced_prob = ifelse(log_unbalanced_pred>.5,1,0)

log_unbalanced_prob <- unname(log_unbalanced_prob)
mean(log_unbalanced_prob == test_data$stroke)

log_unbalanced_prob <- unname(log_unbalanced_prob)

levels(log_unbalanced_prob)=c(0,1)
# log_unbalanced_prob
# test_data$stroke

test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
cm_lr <- confusionMatrix(as.factor(log_unbalanced_prob), as.factor(test_data$stroke), mode = "everything", positive="1")
cm_lr
```


# Logistic on Balanced dataset

```{r}
log_balanced = glm(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = data_balanced_smote, family="binomial")

summary(log_balanced)

log_balanced_pred = predict(log_balanced,test_data,type='response')
log_balanced_prob = ifelse(log_balanced_pred>.5,1,0)

log_balanced_prob <- unname(log_balanced_prob)
mean(log_balanced_prob == test_data$stroke)

log_balanced_prob <- unname(log_balanced_prob)

levels(log_balanced_prob)=c(0,1)


test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
cm_lr_balanced <- confusionMatrix(as.factor(log_balanced_prob), as.factor(test_data$stroke), mode = "everything", positive="1")
cm_lr_balanced
```

## XGBoost on unbalanced data

```{r}
x_train <- data.matrix(subset(train_data, select = -c(stroke)))
x_test <- data.matrix(subset(test_data, select = -c(stroke)))



y_train <- data.matrix(train_data$stroke)
y_test <- data.matrix(test_data$stroke)



dtrain <- xgb.DMatrix(data = x_train, label= y_train)
dtest <- xgb.DMatrix(data = x_test, label= y_test)



negative_cases <- sum(y_train == "0")
postive_cases <- sum(y_train == "1")



stroke_xgboost <- xgboost(data = dtrain,
                          nrounds =10,
                          max.depth = 3,
                          objective = "binary:logistic",
                          early_stopping_rounds = 3,
                          scale_pos_weight = negative_cases/postive_cases)


# prediction
pred <- predict(stroke_xgboost, dtest)
pred_class <- as.integer(pred > 0.4)



# accuracy
test <- ifelse(as.numeric(test_data$stroke) == 2 , 1, 0)
accuracy_xgboost = mean(pred_class == test)
accuracy_xgboost



# confusion matrix
cm_xgboost <- confusionMatrix(as.factor(pred_class), as.factor(test), mode = "everything", positive="1")
cm_xgboost
```


# XGBoost on Balanced data

```{r}
x_train <- data.matrix(subset(data_balanced_smote, select = -c(stroke)))
x_test <- data.matrix(subset(test_data, select = -c(stroke)))



y_train <- data.matrix(data_balanced_smote$stroke)
y_test <- data.matrix(test_data$stroke)



dtrain <- xgb.DMatrix(data = x_train, label= y_train)
dtest <- xgb.DMatrix(data = x_test, label= y_test)



negative_cases <- sum(y_train == "0")
postive_cases <- sum(y_train == "1")



stroke_xgboost <- xgboost(data = dtrain,
nrounds =10,
max.depth = 3,
objective = "binary:logistic",
early_stopping_rounds = 3,
scale_pos_weight = negative_cases/postive_cases)



pred <- predict(stroke_xgboost, dtest)
pred_class <- as.integer(pred > 0.4)



# accuracy
test <- ifelse(as.numeric(test_data$stroke) == 2 , 1, 0)
accuracy_xgboost_balanced = mean(pred_class == test)
accuracy_xgboost_balanced



# confusion matrix
cm_xgboost_balanced <- confusionMatrix(as.factor(pred_class), as.factor(test), mode = "everything", positive="1")
cm_xgboost_balanced
```