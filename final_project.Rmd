---
title: |
    | Methods of Data Science: Final Project 
    | Stroke Prediction based on health and lifestyle factors
    
author: 'Saish Desai, Anish Shetty, Vishnupriya Singh'
output: pdf_document 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# installation of all the packages
# install.packages("ggstatsplot")

# loading all the packages
library("ggstatsplot") 
library(dplyr)
library(mltools)
library(data.table)
library(ROSE)
library(rpart)
library(tree)
library(randomForest)
library(caret)
library(MASS)
library(xgboost)
```
# Introduction

# Research Questions

# Data Cleaning & Analysis

### Dataset Source : https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
```{r}
stroke_data = read.csv("healthcare-dataset-stroke-data.csv")
head(stroke_data)
```
### Dataset Description and EDA

```{r}
str(stroke_data)
```
## Understanding the distribution of variables.

# Categorical Variables

## 1. Gender

```{r}
gender_counts <- as.data.frame(table(stroke_data$gender))

ggplot(gender_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Gender", y = "Frequency")
```

## 2. Hypertension
```{r}
ht_counts <- as.data.frame(table(stroke_data$hypertension))
ht_counts$Var1 <- if_else(ht_counts$Var1 == 0, 'No','Yes')

ggplot(ht_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Hypertension", y = "Frequency")


hypstr_counts <- as.data.frame(table(stroke_data$hypertension, stroke_data$stroke))
hypstr_counts$Var1 <- if_else(hypstr_counts$Var1 == 0, 'No','Yes')
hypstr_counts$Var2 <- if_else(hypstr_counts$Var2 == 0, 'No','Yes')

ggplot(hypstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq), vjust = 0) +
        labs(x ="Hypertension", y = "Frequency", fill = 'Stroke')
```


## 3. Heart Disease

```{r}
hd_counts <- as.data.frame(table(stroke_data$heart_disease))
hd_counts$Var1 <- if_else(hd_counts$Var1 == 0, 'No','Yes')

ggplot(hd_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Heart Disease", y = "Frequency")


hdstr_counts <- as.data.frame(table(stroke_data$heart_disease, stroke_data$stroke))
hdstr_counts$Var1 <- if_else(hdstr_counts$Var1 == 0, 'No','Yes')
hdstr_counts$Var2 <- if_else(hdstr_counts$Var2 == 0, 'No','Yes')

ggplot(hdstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Heart Disease", y = "Frequency", fill = 'Stroke')
```

## 4. Ever Married

```{r}

m_counts <- as.data.frame(table(stroke_data$ever_married))

ggplot(m_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Married?", y = "Frequency")


mstr_counts <- as.data.frame(table(stroke_data$ever_married, stroke_data$stroke))
mstr_counts$Var2 <- if_else(mstr_counts$Var2 == 0, 'No','Yes')

ggplot(mstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Marriage", y = "Frequency", fill = 'Stroke')

```

## 5. Work Type


```{r}

wt_counts <- as.data.frame(table(stroke_data$work_type))

ggplot(wt_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Work Type", y = "Frequency")


wtstr_counts <- as.data.frame(table(stroke_data$work_type, stroke_data$stroke))
wtstr_counts$Var2 <- if_else(wtstr_counts$Var2 == 0, 'No','Yes')

ggplot(wtstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Marriage", y = "Frequency", fill = 'Stroke')

```

## 6. Residence Type

```{r}

rt_counts <- as.data.frame(table(stroke_data$Residence_type))

ggplot(rt_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")


rtstr_counts <- as.data.frame(table(stroke_data$Residence_type, stroke_data$stroke))
rtstr_counts$Var2 <- if_else(rtstr_counts$Var2 == 0, 'No','Yes')

ggplot(rtstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency", fill = 'Stroke')

```

## 7. Smoking Status
```{r}

ss_counts <- as.data.frame(table(stroke_data$smoking_status))

ggplot(ss_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")


ssstr_counts <- as.data.frame(table(stroke_data$smoking_status, stroke_data$stroke))
ssstr_counts$Var2 <- if_else(ssstr_counts$Var2 == 0, 'No','Yes')

ggplot(ssstr_counts, aes(x = Var1, y = Freq, fill = Var2)) +
        geom_bar(stat = "identity")  +
        geom_text(aes(label = Freq)) +
        labs(x ="Smoking Status", y = "Frequency", fill = 'Stroke')

```

# Continous Variables

## 8. Age

```{r}
#reference: https://www.r-bloggers.com/2021/11/how-to-make-stunning-histograms-in-r-a-complete-guide-with-ggplot2/


ggplot(stroke_data, aes(x=age)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```
## 9. Avg Glucose Level

```{r}

ggplot(stroke_data, aes(x=avg_glucose_level)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```



## 10. BMI

```{r}
#reference: https://www.r-bloggers.com/2021/11/how-to-make-stunning-histograms-in-r-a-complete-guide-with-ggplot2/
stroke_data$bmi <- as.integer(stroke_data$bmi)

ggplot(stroke_data, aes(x=bmi)) + 
  geom_histogram(aes(y = ..density..),color = "#000000", fill = "#0099F8") +
  geom_density(color = "#000000", fill = "#F85700", alpha = 0.6)
```

## 11. Stroke

```{r}
s_counts <- as.data.frame(table(stroke_data$stroke))

ggplot(s_counts, aes(x = Var1, y = Freq, fill = Var1)) +
        geom_bar(stat = "identity") + theme(legend.position="none") +
        geom_text(aes(label = Freq)) +
        labs(x ="Residence Type", y = "Frequency")
```
As we can see from the above graph, Our dependant variable stroke is highly imbalanced, Which is true in a realistic scenario as for a sample population, Number of people suffering from stroke is less.

Factorizing all the categorical variables

```{r}
# reference for one hot coding - https://datatricks.co.uk/one-hot-encoding-in-r-three-simple-methods

stroke_data$gender = factor(stroke_data$gender,levels = c('Male', 'Female'),labels = c(0,1))
stroke_data$ever_married = factor(stroke_data$ever_married,levels = c('No', 'Yes'),labels = c(0,1))
stroke_data$Residence_type = factor(stroke_data$Residence_type,levels = c('Rural', 'Urban'),labels = c(0,1))

stroke_data$hypertension = factor(stroke_data$hypertension,levels = c('0', '1'),labels = c(0,1))

stroke_data$heart_disease = factor(stroke_data$heart_disease,levels = c('0', '1'),labels = c(0,1))

stroke_data$smoking_status = factor(stroke_data$smoking_status,levels = c("formerly smoked", "never smoked", "smokes","Unknown"),labels = c(0,1,2,3))
stroke_data$work_type = factor(stroke_data$work_type,levels = c("children", "Govt_job", "Never_worked", "Private","Self-employed"),labels = c(0,1,2,3,4))

stroke_data['bmi'] <- as.numeric(stroke_data$bmi)
stroke_data$stroke<-as.factor(stroke_data$stroke)
```


### Check for NA's in the Dataset
```{r}
#Columnwise percentage of rows which are NA
colMeans(is.na(stroke_data))*100
```


```{r}
mean_bmi <- mean(na.omit(stroke_data$bmi))
median_bmi <- median(na.omit(stroke_data$bmi))

# estimating the mode value of the bmi column
# reference - https://www.tutorialspoint.com/r/r_mean_median_mode.htm

getmode <- function(v) {
   uniqv <- unique(v)
   index <-which.max(tabulate(match(v, uniqv))) #index of the most occuring value
   uniqv[index]
}

mode_bmi <- getmode(na.omit(stroke_data$bmi))


print(mean_bmi)
print(median_bmi)
print(mode_bmi)

# distribution for BMI
d <- density(na.omit(stroke_data$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(stroke_data$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(stroke_data$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")

```

Imputation of missing values with mean of the values from the column 'bmi'
```{r}
stroke_data$bmi[is.na(stroke_data$bmi)] <- mean(stroke_data$bmi, na.rm = TRUE)
stroke_data <- na.omit(stroke_data)
str(stroke_data)
```
### Checking for outliers

```{r}
#boxplot before outlier removal
boxplot(stroke_data$age, main="Age distribution",
   xlab="ID", ylab="Age")

boxplot(stroke_data$bmi, main="Bmi distribution",
   xlab="ID", ylab="bmi")

boxplot(stroke_data$avg_glucose_level, main="Avg glucose level distribution",
   xlab="ID", ylab="avg_glucose_level")
```

### Outlier removal based on IQR
```{r}
#IQR

Q_age <- quantile(stroke_data$age, probs=c(.25, .75), na.rm = FALSE)
iqr_age <- IQR(stroke_data$age)

Q_bmi<- quantile(stroke_data$bmi, probs=c(.25, .75), na.rm = FALSE)
iqr_bmi <- IQR(stroke_data$bmi)
 
Q_avg_glucose_level<- quantile(stroke_data$avg_glucose_level, probs=c(.25, .75), na.rm = FALSE)
iqr_avg_glucose_level <- IQR(stroke_data$avg_glucose_level)


stroke_data_clean<- subset(stroke_data, 
                    stroke_data$age > (Q_age[1] - 1.5*iqr_age) & 
                    stroke_data$age < (Q_age[2] + 1.5*iqr_age) & 
                    stroke_data$bmi > (Q_bmi[1] - 1.5*iqr_bmi) & 
                    stroke_data$bmi < (Q_bmi[2] + 1.5*iqr_bmi) & 
                    stroke_data$avg_glucose_level > (Q_avg_glucose_level[1] - 1.5*iqr_avg_glucose_level) &
                    stroke_data$avg_glucose_level < (Q_avg_glucose_level[2]+1.5*iqr_avg_glucose_level))

str(stroke_data_clean)
str(stroke_data)
```


```{r}
#boxplot after outlier removal
boxplot(stroke_data_clean$age, main="Age distribution",
   xlab="ID", ylab="Age")

boxplot(stroke_data_clean$bmi, main="Bmi distribution",
   xlab="ID", ylab="bmi")

boxplot(stroke_data_clean$avg_glucose_level, main="Avg glucose level distribution",
   xlab="ID", ylab="avg_glucose_level")
```




```{r}
# # min max standardization function
# fun_range <- function(x) {                             
#   (x - min(x)) / (max(x) - min(x))
# }
# # perform normalization
# stroke_data_clean$age <- fun_range(x = stroke_data_clean$age)
# stroke_data_clean$bmi <- fun_range(x = stroke_data_clean$bmi)
# stroke_data_clean$avg_glucose_level <- fun_range(x = stroke_data_clean$avg_glucose_level)
```

# Modeling


```{r}

# train test split

train <- sample(1:nrow(stroke_data_clean),nrow(stroke_data_clean)*0.7)

train_data <- stroke_data_clean[train, ]
test_data <- stroke_data_clean[-train, ]

rf <- randomForest(stroke ~., data = train_data, mtry = sqrt(ncol(train_data) - 1), ntree = 500)


# predicting the income value
yhat_rf <- predict(rf, test_data[,-12])

# accuracy
acc_rf = mean(yhat_rf == test_data$stroke)

# classification metrics
cm <- confusionMatrix(yhat_rf, test_data$stroke, mode = "everything", positive="1")
cm
```
Due to a high class imbalance all the entries are classified as belonging to class 0. This will lead to Precision and Recall value of "0", thus making the F1-score undefined.

# Oversampling to handle the imbalance in the data


# ROSE

```{r}
# data <- ROSE(stroke~., stroke_data, p=0.5, hmult.majo=1, hmult.mino=1)$data
data_balanced_over <- ovun.sample(stroke~ ., data = train_data, p=0.6, method = "over")$data
table(data_balanced_over$stroke)
```

The data is highly imbalanced due to the fact the there are very few people having suffered from stroke. So to reduce the class imbalance we use over smapling techniques to replicate some data with affecting the probabilty distribution of predictors in the data set. 

But, before that we need to remove the outliers from the dataset.



```{r}
data_balanced <-  data_balanced_over
# data_balanced
table(data_balanced$stroke)
```
## Distribution of Numerical variables in the Unbalanced Dataset
```{r}
# distribution of balanced data

# distribution for BMI
d <- density(na.omit(data_balanced$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(data_balanced$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(data_balanced$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")
```


```{r}

wn = sum(data_balanced$stroke =="0")/length(data_balanced$stroke)
wy = 1

rf <- randomForest(stroke ~., data = data_balanced, 
                   mtry =sqrt(ncol(data_balanced) - 1), 
                   classwt = c("0"=wn, "1"=wy), ntree = 500)


# predicting the income value
yhat_rf <- predict(rf, test_data[,-14])

# accuracy
acc_rf = mean(yhat_rf == test_data$stroke)

# classification metrics
cm <- confusionMatrix(yhat_rf, test_data$stroke, mode = "everything", positive="1")
cm
```


Applying random forest to the data
```{r}
table(test_data$stroke)
```

# SMOTE
Another way to sample the data is SMOTE which results in minority sampling. 
Applying the same on training data, we get the following results.
Before that, data should be converted to numeric first.
```{r}
cols<- c("gender", "hypertension", "heart_disease", "ever_married", "work_type", "Residence_type","smoking_status" )
train_data[cols]<-lapply(train_data[cols], as.numeric)
test_data[cols]<-lapply(test_data[cols], as.numeric)
```

```{r}
library(imbalance)
#train_data<-train_data[,-1]
#test_data<-test_data[,-1]
data_balanced_smote<-oversample(train_data, ratio = 0.6, method = "SMOTE", classAttr = "stroke")
table(data_balanced_smote$stroke)
```

Here SMOTE adds few rpws to the training dataset, reproducing minority(1) factor level.

#Distribution of Numerical variables after Balancing the dataset

```{r}
# distribution for BMI
d <- density(na.omit(data_balanced_smote$bmi))
plot(d, main="Distribution for BMI ")
polygon(d, col="green", border="blue")

# distribution for age
d <- density(na.omit(data_balanced_smote$age))
plot(d, main="Distribution for Age ")
polygon(d, col="red", border="blue")

# distribution for avg_glucose_level
d <- density(na.omit(data_balanced_smote$avg_glucose_level))
plot(d, main="Distribution for Avg_glucose_level")
polygon(d, col="yellow", border="blue")
```

# Random Forest on the Balanced Dataset

```{r}
library(randomForest)
data_balanced_smote$stroke<-as.factor(data_balanced_smote$stroke)
rf <- randomForest(stroke ~., data = data_balanced_smote, mtry =sqrt(ncol(data_balanced_smote) - 1), ntree = 500)
# predicting the income value
yhat_rf <- predict(rf, test_data)
# accuracy
acc_rf = mean(yhat_rf == test_data$stroke)
cm <- confusionMatrix(yhat_rf, as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```
# Applying bagging and boosting
```{r}
library(randomForest)
data_balanced_smote$stroke<-as.factor(data_balanced_smote$stroke)
rf <- randomForest(stroke ~., data = data_balanced_smote, mtry =ncol(data_balanced_smote) - 1, ntree = 500)
# predicting the income value
yhat_rf <- predict(rf, test_data)
# accuracy
acc_rf = mean(yhat_rf == test_data$stroke)
cm <- confusionMatrix(yhat_rf, as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```
 
# Boosting
```{r}
data_balanced_smote$stroke=as.numeric(data_balanced_smote$stroke)
data_balanced_smote$stroke = as.numeric(ifelse(data_balanced_smote$stroke == 1, "0", "1"))
set.seed (1)
library (gbm)
boost.data <- gbm(stroke~ ., data = data_balanced_smote,
distribution = "bernoulli", n.trees = 500,
interaction.depth = 4)
a = summary(predict(boost.data,test_data))
a
yhat.boost.test <- ifelse(predict(boost.data ,newdata = test_data, n.trees =500)>a[4],1,0) #min+max/2
acc_boost = mean(yhat.boost.test == test_data$stroke)
cm <- confusionMatrix(as.factor(yhat.boost.test), as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```

# SVM on Balanced Dataset
```{r}
library(e1071)
svmfit <- svm ( stroke ~ ., data = data_balanced_smote , kernel = "linear",
cost = 0.5, scale = FALSE)

```

```{r}
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
test.pred <- predict (svmfit , test_data)
# accuracy
acc_rf = mean(test.pred == test_data$stroke)
cm <- confusionMatrix(test.pred, as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```

# Naive Bayes on Unbalanced Dataset by using prior = 1
```{r}
set.seed(1)
library(e1071)
nbfit <- naiveBayes ( stroke ~ ., data = train_data, prior = 1)
```


```{r}
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
test.pred <- predict (nbfit , test_data)
# accuracy
acc_rf = mean(test.pred == test_data$stroke)
acc_rf
cm <- confusionMatrix(test.pred, as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```


# Naive Bayes on Balanced Dataset by using prior = 1
```{r}
set.seed(1)
library(e1071)
nbfit_balanced <- naiveBayes ( stroke ~ ., data = data_balanced_smote, prior = 1)
```


```{r}
test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
nbfit_balanced_pred <- predict(nbfit , test_data)
# accuracy
acc_rf = mean(nbfit_balanced_pred == test_data$stroke)
acc_rf
cm <- confusionMatrix(test.pred, as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```



## LDA on Unbalanced Dataset

```{r}
lda_unbalanced <- lda(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_data)

lda_unbalanced
```

```{r}
lda_unbalanced_predict <- predict(lda_unbalanced, test_data)
lda_class <- lda_unbalanced_predict$class

table(lda_class , test_data$stroke)
mean(lda_unbalanced_predict$class != test_data$stroke)

test_data$stroke = as.factor(test_data$stroke)
cm <- confusionMatrix(lda_unbalanced_predict$class, test_data$stroke, mode = "everything", positive="1")
cm
```



## LDA on Balanced dataset

```{r}
lda_balanced <- lda(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = data_balanced_smote)

lda_balanced
```

```{r}
lda_balanced_predict <- predict(lda_balanced, test_data)
lda_class <- lda_balanced_predict$class

table(lda_class , test_data$stroke)
mean(lda_balanced_predict$class != test_data$stroke)

test_data$stroke = as.factor(test_data$stroke)
cm <- confusionMatrix(lda_balanced_predict$class, test_data$stroke, mode = "everything", positive="1")
cm
```

#Logistic on Unbalanced Dataset

```{r}
log_unbalanced = glm(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = train_data, family="binomial")

summary(log_unbalanced)
```
```{r}
log_unbalanced_pred = predict(log_unbalanced,test_data,type='response')
log_unbalanced_prob = ifelse(log_unbalanced_pred>.5,1,0)

log_unbalanced_prob <- unname(log_unbalanced_prob)
mean(log_unbalanced_prob == test_data$stroke)

log_unbalanced_prob <- unname(log_unbalanced_prob)

levels(log_unbalanced_prob)=c(0,1)
# log_unbalanced_prob
# test_data$stroke

test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
cm <- confusionMatrix(as.factor(log_unbalanced_prob), as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```


# Logistic on Balanced dataset

```{r}
log_balanced = glm(stroke ~ gender+age+hypertension+heart_disease+ever_married+work_type+Residence_type+avg_glucose_level+bmi+smoking_status, data = data_balanced_smote, family="binomial")

summary(log_balanced)
```


```{r}
log_balanced_pred = predict(log_balanced,test_data,type='response')
log_balanced_prob = ifelse(log_balanced_pred>.5,1,0)

log_balanced_prob <- unname(log_balanced_prob)
mean(log_balanced_prob == test_data$stroke)

log_balanced_prob <- unname(log_balanced_prob)

levels(log_balanced_prob)=c(0,1)


test_data$stroke<- as.factor(test_data$stroke)
levels(test_data$stroke)=c(0,1)
cm <- confusionMatrix(as.factor(log_balanced_prob), as.factor(test_data$stroke), mode = "everything", positive="1")
cm
```

## XGBoost on unbalanced data

```{r}
x_train <- data.matrix(subset(train_data, select = -c(stroke)))
x_test <- data.matrix(subset(test_data, select = -c(stroke)))



y_train <- data.matrix(train_data$stroke)
y_test <- data.matrix(test_data$stroke)



dtrain <- xgb.DMatrix(data = x_train, label= y_train)
dtest <- xgb.DMatrix(data = x_test, label= y_test)



negative_cases <- sum(y_train == "0")
postive_cases <- sum(y_train == "1")



stroke_xgboost <- xgboost(data = dtrain,
nrounds =10,
max.depth = 3,
objective = "binary:logistic",
early_stopping_rounds = 3,
scale_pos_weight = negative_cases/postive_cases)



pred <- predict(stroke_xgboost, dtest)
pred_class <- as.integer(pred > 0.4)



# accuracy
test <- ifelse(as.numeric(test_data$stroke) == 2 , 1, 0)
accuracy_xgboost = mean(pred_class == test)
accuracy_xgboost



# confusion matrix
cm_xgboost <- confusionMatrix(as.factor(pred_class), as.factor(test), mode = "everything", positive="1")
cm_xgboost
```


# XGBoost on Balanced data

```{r}
x_train <- data.matrix(subset(data_balanced_smote, select = -c(stroke)))
x_test <- data.matrix(subset(test_data, select = -c(stroke)))



y_train <- data.matrix(data_balanced_smote$stroke)
y_test <- data.matrix(test_data$stroke)



dtrain <- xgb.DMatrix(data = x_train, label= y_train)
dtest <- xgb.DMatrix(data = x_test, label= y_test)



negative_cases <- sum(y_train == "0")
postive_cases <- sum(y_train == "1")



stroke_xgboost <- xgboost(data = dtrain,
nrounds =10,
max.depth = 3,
objective = "binary:logistic",
early_stopping_rounds = 3,
scale_pos_weight = negative_cases/postive_cases)



pred <- predict(stroke_xgboost, dtest)
pred_class <- as.integer(pred > 0.4)



# accuracy
test <- ifelse(as.numeric(test_data$stroke) == 2 , 1, 0)
accuracy_xgboost = mean(pred_class == test)
accuracy_xgboost



# confusion matrix
cm_xgboost <- confusionMatrix(as.factor(pred_class), as.factor(test), mode = "everything", positive="1")
cm_xgboost
```